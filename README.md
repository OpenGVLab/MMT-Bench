# MMT-Bench

This repository is the official implementation of [MMT-Bench](xxx). 

> [MMT-Bench: A Multimodal MultiTask Benchmark for Comprehensive Evaluation of Large Vision-Language Models](xxx)  
> Kaining Ying<sup>\*</sup>, Fanqing Meng<sup>\*</sup>, Jin Wang<sup>\*</sup>, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, jiayi lei, Quanfeng Lu, Peng Gao, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang<sup>\#</sup>, Wenqi Shao<sup>\#</sup>  
> <sup>\*</sup> KY, FM and JW contribute equally. <sup>\#</sup> KZ (zhangkaipeng@pjlab.org.cn) and WS (shaowenqi@pjlab.org.cn) are correponding authors. 

![overview](assets/overview.jpg)

## üèÜ Leaderboard

| Rank | Model                 | Overall |
|------|-----------------------|---------|
| 1    | InternVL-Chat-v1.2    | 63.4    |
| 2    | Qwen-VL-Plus          | 62.3    |
| 3    | GPT-4V                | 62.0    |
| 4    | GeminiProVision       | 61.6    |
| 5    | LLaVA-NEXT-34B        | 60.8    |
| 6    | XComposer2            | 55.7    |
| 7    | BLIP2                 | 54.8    |
| 8    | Yi-VL-34B             | 54.2    |
| 9    | Monkey-Chat           | 53.4    |
| 10   | DeepSeek-VL-7B        | 53.2    |
| 11   | Yi-VL-6B              | 53.2    |
| 12   | LLaVA-NEXT-13B        | 53.0    |
| 13   | TransCore-M           | 52.7    |
| 14   | QWen-VL-Chat          | 52.5    |
| 15   | Claude3V-Haiku        | 52.2    |
| 16   | XComposer             | 52.1    |
| 17   | mPLUG-Owl2            | 52.0    |
| 18   | RBDash-v1-13B         | 51.8    |
| 19   | LLaVA-v1.5-13B        | 51.7    |
| 20   | CogVLM-Chat           | 51.6    |
| 21   | ShareGPT4V-7B         | 51.5    |
| 22   | LLaVA-NEXT-7B         | 51.1    |
| 23   | LLaVA-v1.5-13B-XTuner | 51.1    |
| 24   | LLaVA-InternLM2-7B    | 50.8    |
| 25   | LLaVA-v1.5-7B-XTuner  | 50.2    |
| 26   | SharedCaptioner       | 49.9    |
| 27   | LLaVA-InternLM-7B     | 49.7    |
| 28   | LLaVA-v1.5-7B         | 49.5    |
| 29   | LLaMA-Adapter-v2-7B   | 40.4    |
| 30   | VisualGLM-6B          | 38.6    |
| 31   | Frequency Guess       | 31.7    |
| 32   | Random Guess          | 28.5    |



## üí° News



## üöÄ Quick Start

Please refer to [this](Quickstart.md) to quick start.


## üíê Acknowledgement

We expressed sincerely gratitude for the projects listed following:
- [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) provides useful out-of-box tools and implements many adavanced LVLMs.


## üñäÔ∏è Citation 
If you feel MMT-Bench useful in your project or research, please kindly use the following BibTeX entry to cite our paper. Thanks!
```
xxx
```